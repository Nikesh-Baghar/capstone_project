# -*- coding: utf-8 -*-
"""text_generation_streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O1zrzb7MzR6mzYLIWqJ2ECxYQHbIvokA
"""

# streamlit_app.py
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch

# -----------------------------
# Model Configurations
# -----------------------------
MODELS = {
    "MistralLite": "amazon/MistralLite",
    "GPT-Neo (125M)": "EleutherAI/gpt-neo-125M",
    "GPT-2": "openai-community/gpt2",
    "Qwen-0.6B": "Qwen/Qwen3-0.6B"
}

# -----------------------------
# Load model and tokenizer
# -----------------------------
@st.cache_resource
def load_model(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map="auto"
    )
    return tokenizer, model

# -----------------------------
# Generation Function
# -----------------------------
def generate_response(tokenizer, model, prompt, strategy="greedy",
                      temperature=0.7, top_k=20, top_p=0.85, max_new_tokens=800):

    gen_kwargs = {"max_new_tokens": max_new_tokens, "temperature": temperature}

    if strategy == "greedy":
        gen_kwargs.update({"do_sample": False})
    elif strategy == "beam":
        gen_kwargs.update({"num_beams": 5, "do_sample": False})
    elif strategy == "top-k":
        gen_kwargs.update({"do_sample": True, "top_k": top_k})
    elif strategy == "top-p":
        gen_kwargs.update({"do_sample": True, "top_p": top_p})
    else:
        raise ValueError("Choose from: greedy, beam, top-k, top-p")

    # Handle Qwen (chat template)
    if "Qwen" in model.name_or_path and hasattr(tokenizer, "apply_chat_template"):
        messages = [{"role": "user", "content": prompt}]
        text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True, enable_thinking=False
        )
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
    else:
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(**inputs, **gen_kwargs)
    result = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    return result.strip()

# -----------------------------
# Streamlit UI
# -----------------------------
st.title("LLM Response Comparator")

# User selections
model_choice = st.selectbox("Select a model:", list(MODELS.keys()))
strategy = st.selectbox("Select decoding strategy:", ["greedy", "beam", "top-k", "top-p"])
prompt = st.text_area("Enter your prompt:", "Write a 500-word essay on the future of AI.")

temperature = st.slider("Temperature", 0.0, 2.0, 1.0, 0.1)
max_new_tokens = st.slider("Max New Tokens", 50, 800, 500, 50)

if st.button("Generate"):
    tokenizer, model = load_model(MODELS[model_choice])
    with st.spinner("Generating response..."):
        response = generate_response(
            tokenizer, model, prompt, strategy=strategy,
            temperature=temperature, max_new_tokens=max_new_tokens
        )
    st.subheader("Generated Response")
    st.write(response)
