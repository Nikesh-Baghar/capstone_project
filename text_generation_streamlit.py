# -*- coding: utf-8 -*-
"""text_generation_streamlit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O1zrzb7MzR6mzYLIWqJ2ECxYQHbIvokA
"""

# streamlit_app.py
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# -----------------------------
# Model Configurations
# -----------------------------
MODELS = {
    "GPT-Neo (125M)": "EleutherAI/gpt-neo-125M",
    "GPT-2": "openai-community/gpt2",
    "Qwen-0.6B": "Qwen/Qwen3-0.6B"
}

# -----------------------------
# Load model and tokenizer
# -----------------------------
@st.cache_resource
def load_model(model_name):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype="auto",
        device_map="auto"
    )
    return tokenizer, model


# -----------------------------
# Generation Function
# -----------------------------
def generate_response(tokenizer, model, prompt, strategy="greedy",
                      temperature=0.7, top_k=20, top_p=0.85, max_new_tokens=800):
    """
    Robust generation:
      - checks tokenizer/model max length
      - avoids silent truncation
      - sets max_length = min(model_max_len, input_len + max_new_tokens)
    """

    gen_kwargs = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "do_sample": True
    }

    # --- Decoding Strategies ---
    if strategy == "greedy":
        gen_kwargs.update({"do_sample": False})
    elif strategy == "beam":
        gen_kwargs.update({"num_beams": 5, "do_sample": False})
    elif strategy == "top-k":
        gen_kwargs.update({"top_k": top_k})
    elif strategy == "top-p":
        gen_kwargs.update({"top_p": top_p})
    elif strategy == "contrastive-search":
        gen_kwargs.update({"penalty_alpha": 0.6, "top_k": 4})
    elif strategy == "locally-typical":
        gen_kwargs.update({"typical_p": 0.9})
    elif strategy == "speculative-decoding":
        gen_kwargs.update({"top_p": 0.8, "temperature": 0.6, "top_k": 30})
    elif strategy == "contrastive-divergence":
        gen_kwargs.update({"repetition_penalty": 1.8, "top_k": 40, "temperature": 0.7})
    else:
        raise ValueError("Unsupported decoding strategy")

    # --- Prepare inputs robustly ---
    # For chat-style models we might use apply_chat_template
    if "Qwen" in model.name_or_path and hasattr(tokenizer, "apply_chat_template"):
        messages = [{"role": "user", "content": prompt}]
        text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True, enable_thinking=False
        )
        text_to_tokenize = text
    else:
        text_to_tokenize = prompt

    # Tokenize WITHOUT silent truncation, then handle too-long input explicitly
    # Note: tokenizer.model_max_length is usually the safe upper bound
    model_max_len = getattr(tokenizer, "model_max_length", None)
    if model_max_len is None:
        # fallback to model config if tokenizer doesn't expose it
        model_max_len = getattr(model.config, "max_position_embeddings", None)

    # Tokenize with no truncation so we can check length ourselves
    inputs = tokenizer(
        text_to_tokenize,
        return_tensors="pt",
        truncation=False,  # don't let tokenizer silently chop it
        padding=False
    ).to(model.device)

    input_len = inputs.input_ids.shape[1]

    # If the prompt itself is longer than allowed context, truncate intelligently (keep end)
    if model_max_len is not None and input_len >= model_max_len:
        # keep the tail of the prompt (most recent tokens), so instruction remains near generation point
        # Tokenize again but now allow truncation from the left by using `max_length` and `truncation='only_first'` behavior:
        # Some tokenizers don't have 'truncation' options for direction; easiest is to re-tokenize with max_length
        st.warning(f"Prompt too long for model context ({input_len} tokens > {model_max_len}). Truncating to last {model_max_len - 1} tokens.")
        # Re-tokenize keeping last tokens: convert to ids then slice
        all_ids = inputs.input_ids[0]
        keep = model_max_len - 1  # reserve at least 1 token for generation start
        new_ids = all_ids[-keep:].unsqueeze(0).to(model.device)
        inputs['input_ids'] = new_ids
        # no attention mask? create one
        inputs['attention_mask'] = torch.ones_like(inputs['input_ids']).to(model.device)
        input_len = inputs.input_ids.shape[1]

    # Compute safe max_length for generation
    if model_max_len is not None:
        max_allowed = min(model_max_len, input_len + max_new_tokens)
    else:
        max_allowed = input_len + max_new_tokens

    # Pass both max_length and max_new_tokens to be explicit
    gen_kwargs.update({"max_length": max_allowed, "max_new_tokens": max_new_tokens})

    # Generate
    with torch.no_grad():
        outputs = model.generate(**inputs, **gen_kwargs)

    # outputs[0] is full sequence: input_ids + generated tokens in most HF models
    generated_ids = outputs[0][input_len:]
    result = tokenizer.decode(generated_ids, skip_special_tokens=True)
    return result.strip()



# -----------------------------
# Streamlit UI
# -----------------------------
st.title("ðŸ§  LLM Response Comparator")

# Model & decoding selections
model_choice = st.selectbox("Select a model:", list(MODELS.keys()))

strategy = st.selectbox(
    "Select decoding strategy:",
    [
        "greedy",
        "beam",
        "top-k",
        "top-p",
        "contrastive-search",
        "locally-typical",
        "speculative-decoding",
        "contrastive-divergence"
    ]
)

# -----------------------------
# Task Selection
# -----------------------------
task = st.selectbox(
    "Select a task:",
    ["Machine Translation", "Summarization", "Story Generation"]
)

if task == "Machine Translation":
    translation_dir = st.selectbox(
        "Select translation direction:",
        ["English â†’ German", "German â†’ English"]
    )
    prompt_label = f"Enter your text to translate ({translation_dir}):"
elif task == "Summarization":
    prompt_label = "Enter the text to summarize:"
else:
    prompt_label = "Enter the story prompt:"

prompt = st.text_area(prompt_label, "")

# -----------------------------
# Generation Settings
# -----------------------------
temperature = st.slider("Temperature", 0.0, 2.0, 1.0, 0.1)
max_new_tokens = st.slider("Max New Tokens", 50, 800, 500, 50)

# -----------------------------
# Generate Button
# -----------------------------
if st.button("Generate"):
    tokenizer, model = load_model(MODELS[model_choice])
    with st.spinner("Generating response..."):
        if task == "Machine Translation":
            if translation_dir == "English â†’ German":
                final_prompt = f"{prompt}:Translate the text from English to German language"
            else:
                final_prompt = f"{prompt}:Translate the text from German to English language"
        elif task == "Summarization":
            final_prompt = f"{prompt}:Summarize the text in 80 words"
        elif task == "Story Generation":
            final_prompt = f"{prompt}Continue the story creatively"
        else:
            final_prompt = prompt  
        
        response = generate_response(
            tokenizer, model, final_prompt,
            strategy=strategy,
            temperature=temperature,
            max_new_tokens=max_new_tokens
        )

    st.subheader("Generated Response")
    st.write(response)
